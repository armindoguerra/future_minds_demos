{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de6b277-c2f8-4f78-8890-39a1e0f1feac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0f9783-5673-4558-bb54-60532fdffe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install git+https://github.com/brandonstarxel/chunking_evaluation.git\n",
    "# !pip install watermark\n",
    "# !pip install langchain-community langchain-core\n",
    "# !pip install -U langchain-openai\n",
    "# !pip install -U weaviate-client\n",
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43355a4-5b06-48e5-8a21-8e7b83ce6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import weaviate\n",
    "\n",
    "from chunking_evaluation.chunking import RecursiveTokenChunker\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from weaviate.classes.config import Configure\n",
    "\n",
    "from credentials import OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db374f10-a5df-4089-84b1-520ea76f7356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Armindo Guerra\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Armindo Guerra\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e9bf8d-6bba-4756-b13a-96a2aafe3c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPython            : 8.27.0\n",
      "langchain_core     : 0.3.47\n",
      "weaviate           : 4.11.1\n",
      "langchain_community: 0.3.20\n",
      "chunking_evaluation: 0.1.0\n",
      "langchain_openai   : 0.3.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33d235-5425-4b37-b431-d5d50266f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY # load your open ai key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab19b99-dd6d-4fff-9222-879352ee8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ca109-61a3-48d2-b527-4a1baf951a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate weaviate database client\n",
    "vdb_client = weaviate.connect_to_local()\n",
    "print(\"Connected to Weviate: \", vdb_client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813d81f-dfee-4481-9247-7d6d033b71ad",
   "metadata": {},
   "source": [
    "# Simulando uma primeira interação com o Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d3fa7-2109-449b-95f6-077b10622704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define System Prompt\n",
    "system_prompt = SystemMessage(\"You are a helpful AI Assistant. Answer the User's queries succinctly in one sentence.\")\n",
    "\n",
    "# Start Storage for Historical Message History\n",
    "messages = [system_prompt]\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Get User's Message\n",
    "    user_message = HumanMessage(input(\"\\nUser: \"))\n",
    "    \n",
    "    if user_message.content.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        # Extend Messages List With User Message\n",
    "        messages.append(user_message)\n",
    "\n",
    "    # Pass Entire Message Sequence to LLM to Generate Response\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    print(\"\\nAI Message: \", response.content)\n",
    "\n",
    "    # Add AI's Response to Message List\n",
    "    messages.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f81bc9f-69d1-4aa3-8fe6-2cc5e6dbf8cd",
   "metadata": {},
   "source": [
    "# Definindo o prompt para o processo de reflexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c234c48f-54d9-4bc7-b620-88c7c38d1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt_template = \"\"\"\n",
    "You are analyzing conversations about research papers to create memories that will help guide future interactions. \n",
    "Your task is to extract key elements that would be most helpful when encountering similar academic discussions in the future.\n",
    "\n",
    "Review the conversation and create a memory reflection following these rules:\n",
    "\n",
    "1. For any field where you don't have enough information or the field isn't relevant, use \"N/A\"\n",
    "2. Be extremely concise - each string should be one clear, actionable sentence\n",
    "3. Focus only on information that would be useful for handling similar future conversations\n",
    "4. Context_tags should be specific enough to match similar situations but general enough to be reusable\n",
    "\n",
    "Output valid JSON in exactly this format:\n",
    "{{\n",
    "    \"context_tags\": [              // 2-4 keywords that would help identify similar future conversations\n",
    "        string,                    // Use field-specific terms like \"deep_learning\", \"methodology_question\", \"results_interpretation\"\n",
    "        ...\n",
    "    ],\n",
    "    \"conversation_summary\": string, // One sentence describing what the conversation accomplished\n",
    "    \"what_worked\": string,         // Most effective approach or strategy used in this conversation\n",
    "    \"what_to_avoid\": string        // Most important pitfall or ineffective approach to avoid\n",
    "}}\n",
    "\n",
    "Examples:\n",
    "- Good context_tags: [\"transformer_architecture\", \"attention_mechanism\", \"methodology_comparison\"]\n",
    "- Bad context_tags: [\"machine_learning\", \"paper_discussion\", \"questions\"]\n",
    "\n",
    "- Good conversation_summary: \"Explained how the attention mechanism in the BERT paper differs from traditional transformer architectures\"\n",
    "- Bad conversation_summary: \"Discussed a machine learning paper\"\n",
    "\n",
    "- Good what_worked: \"Using analogies from matrix multiplication to explain attention score calculations\"\n",
    "- Bad what_worked: \"Explained the technical concepts well\"\n",
    "\n",
    "- Good what_to_avoid: \"Diving into mathematical formulas before establishing user's familiarity with linear algebra fundamentals\"\n",
    "- Bad what_to_avoid: \"Used complicated language\"\n",
    "\n",
    "Additional examples for different research scenarios:\n",
    "\n",
    "Context tags examples:\n",
    "- [\"experimental_design\", \"control_groups\", \"methodology_critique\"]\n",
    "- [\"statistical_significance\", \"p_value_interpretation\", \"sample_size\"]\n",
    "- [\"research_limitations\", \"future_work\", \"methodology_gaps\"]\n",
    "\n",
    "Conversation summary examples:\n",
    "- \"Clarified why the paper's cross-validation approach was more robust than traditional hold-out methods\"\n",
    "- \"Helped identify potential confounding variables in the study's experimental design\"\n",
    "\n",
    "What worked examples:\n",
    "- \"Breaking down complex statistical concepts using visual analogies and real-world examples\"\n",
    "- \"Connecting the paper's methodology to similar approaches in related seminal papers\"\n",
    "\n",
    "What to avoid examples:\n",
    "- \"Assuming familiarity with domain-specific jargon without first checking understanding\"\n",
    "- \"Over-focusing on mathematical proofs when the user needed intuitive understanding\"\n",
    "\n",
    "Do not include any text outside the JSON object in your response.\n",
    "\n",
    "Here is the prior conversation:\n",
    "\n",
    "{conversation}\n",
    "\"\"\"\n",
    "\n",
    "reflection_prompt = ChatPromptTemplate.from_template(reflection_prompt_template)\n",
    "\n",
    "llm_reflection = ChatOpenAI(temperature=0.7, model=\"gpt-4o\")\n",
    "\n",
    "reflect = reflection_prompt | llm_reflection | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b3302-cd69-4e4a-9b44-eaf3209394bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(messages):\n",
    "    \n",
    "    # Create an empty list placeholder\n",
    "    conversation = []\n",
    "    \n",
    "    # Start from index 1 to skip the first system message\n",
    "    for message in messages[1:]:\n",
    "        conversation.append(f\"{message.type.upper()}: {message.content}\")\n",
    "    \n",
    "    # Join with newlines\n",
    "    return \"\\n\".join(conversation)\n",
    "    # return conversation\n",
    "\n",
    "conversation = format_conversation(messages)\n",
    "\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce260e05-fc34-4e89-8c0a-9dc288888da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection = reflect.invoke({\"conversation\": conversation})\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7224de-f9a5-4822-9caf-2151088b351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb_client.collections.create(\n",
    "    \"episodic_memory\",\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_ollama(\n",
    "            name=\"title_vector\",\n",
    "            source_properties=[\"title\"],\n",
    "            api_endpoint=\"http://host.docker.internal:11434\",  # If using Docker, use this to contact your local Ollama instance\n",
    "            model=\"nomic-embed-text\",  # The model to use, e.g. \"nomic-embed-text\"\n",
    "        )\n",
    "    ],\n",
    "    # Additional parameters not shown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40be100-40d5-46e7-9112-1656d65443d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_episodic_memory(messages, vdb_client):\n",
    "\n",
    "    # Format Messages\n",
    "    conversation = format_conversation(messages)\n",
    "\n",
    "    # Create Reflection\n",
    "    reflection = reflect.invoke({\"conversation\": conversation})\n",
    "\n",
    "    collection = vdb_client.collections.get(\"episodic_memory\")\n",
    "\n",
    "    with collection.batch.dynamic() as batch:\n",
    "        batch.add_object(\n",
    "            properties={\n",
    "                \"conversation\": conversation,\n",
    "                \"context_tags\": reflection[\"context_tags\"],\n",
    "                \"conversation_summary\": reflection[\"conversation_summary\"],\n",
    "                \"what_worked\": reflection[\"what_worked\"],\n",
    "                \"what_to_avoid\": reflection[\"what_to_avoid\"],\n",
    "            },\n",
    "        )\n",
    "\n",
    "    failed_objects = collection.batch.failed_objects\n",
    "    if failed_objects:\n",
    "        print(f\"Number of failed imports: {len(failed_objects)}\")\n",
    "        print(f\"First failed object: {failed_objects[0]}\")\n",
    "    else:\n",
    "        print(f\"Objects inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c78797-1018-4694-abea-9401957379d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_episodic_memory(messages, vdb_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac2001-32e0-461a-849e-344a0dc40eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodic_recall(query, vdb_client):\n",
    "    \n",
    "    # Load Database Collection\n",
    "    episodic_memory = vdb_client.collections.get(\"episodic_memory\")\n",
    "\n",
    "    # Hybrid Semantic/BM25 Retrieval\n",
    "    memory = episodic_memory.query.hybrid(\n",
    "        query=query,\n",
    "        alpha=0.5,\n",
    "        limit=1,\n",
    "    )\n",
    "    \n",
    "    return memory\n",
    "\n",
    "query = \"Talking about my name\"\n",
    "memory = episodic_recall(query, vdb_client)\n",
    "memory.objects[0].properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b326cb1-2217-47e2-beab-b7d81c470efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./coala_paper.pdf\")\n",
    "pages = []\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "# Combine all page contents into one string\n",
    "document = \" \".join(page.page_content for page in pages)\n",
    "\n",
    "# Set up the chunker with your specified parameters\n",
    "recursive_character_chunker = RecursiveTokenChunker(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split the combined text\n",
    "recursive_character_chunks = recursive_character_chunker.split_text(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98047e-d16a-4b55-8a67-a92828ad1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recursive_character_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55bb421-4975-4af6-9e49-c1f73d7cc58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb_client.collections.create(\n",
    "    \"semantic_memory\",\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_ollama(\n",
    "            name=\"title_vector\",\n",
    "            source_properties=[\"title\"],\n",
    "            api_endpoint=\"http://host.docker.internal:11434\",  # If using Docker, use this to contact your local Ollama instance\n",
    "            model=\"nomic-embed-text\",  # The model to use, e.g. \"nomic-embed-text\"\n",
    "        )\n",
    "    ],\n",
    "    # Additional parameters not shown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232e679-a0a2-4124-b29f-719f54ad5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Database Collection\n",
    "coala_collection = vdb_client.collections.get(\"semantic_memory\")\n",
    "\n",
    "with coala_collection.batch.dynamic() as batch_coala:\n",
    "    for chunk in recursive_character_chunks:\n",
    "        # The model provider integration will automatically vectorize the object\n",
    "        batch_coala.add_object(\n",
    "            properties={\n",
    "                \"chunk\": chunk,\n",
    "            },\n",
    "            # vector=vector  # Optionally provide a pre-obtained vector\n",
    "        )\n",
    "        if batch_coala.number_errors > 10:\n",
    "            print(\"Batch import stopped due to excessive errors.\")\n",
    "            break\n",
    "\n",
    "failed_objects = coala_collection.batch.failed_objects\n",
    "if failed_objects:\n",
    "    print(f\"Number of failed imports: {len(failed_objects)}\")\n",
    "    print(f\"First failed object: {failed_objects[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c76a0-1319-4eef-9e8b-6a380f58bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_recall(query, vdb_client):\n",
    "    \n",
    "    # Load Database Collection\n",
    "    coala_collection = vdb_client.collections.get(\"semantic_memory\")\n",
    "\n",
    "    # Hybrid Semantic/BM25 Retrieval\n",
    "    memories = coala_collection.query.hybrid(\n",
    "        query=query,\n",
    "        alpha=0.5,\n",
    "        limit=15,\n",
    "    )\n",
    "\n",
    "    combined_text = \"\"\n",
    "    \n",
    "    for i, memory in enumerate(memories.objects):\n",
    "\n",
    "        combined_text += f\"\\nCHUNK {i+1}:\\n\"\n",
    "        combined_text += memory.properties['chunk'].strip()\n",
    "    \n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7c7f7-7fec-4ba2-9eef-8219fe91f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memories = semantic_recall(\"What is retroformer?\", vdb_client)\n",
    "# print(memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f0f3e-fe48-4d62-ac45-a2bfee4c38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_rag(query, vdb_client):\n",
    "\n",
    "    memories = semantic_recall(query, vdb_client)\n",
    "\n",
    "    semantic_prompt = f\"\"\" If needed, Use this grounded context to factually answer the next question.\n",
    "    Let me know if you do not have enough information or context to answer a question.\n",
    "    \n",
    "    {memories}\n",
    "    \"\"\"\n",
    "    \n",
    "    return HumanMessage(semantic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded6cfc9-0339-40ab-97ab-6b4d90d8b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodic_system_prompt(query, vdb_client):\n",
    "    # Get new memory\n",
    "    memory = episodic_recall(query, vdb_client)\n",
    "    \n",
    "    # Load Existing Procedural Memory Instructions\n",
    "    with open(\"./procedural_memory.txt\", \"r\") as content:\n",
    "        procedural_memory = content.read()\n",
    "    \n",
    "    # Get current conversation\n",
    "    current_conversation = memory.objects[0].properties['conversation']\n",
    "    \n",
    "    # Update memory stores, excluding current conversation from history\n",
    "    if current_conversation not in conversations:\n",
    "        conversations.append(current_conversation)\n",
    "    what_worked.update(memory.objects[0].properties['what_worked'].split('. '))\n",
    "    what_to_avoid.update(memory.objects[0].properties['what_to_avoid'].split('. '))\n",
    "    \n",
    "    # Get previous conversations excluding the current one\n",
    "    previous_convos = [conv for conv in conversations[-4:] if conv != current_conversation][-3:]\n",
    "    \n",
    "    # Create prompt with accumulated history\n",
    "    episodic_prompt = f\"\"\"You are a helpful AI Assistant. Answer the user's questions to the best of your ability.\n",
    "    You recall similar conversations with the user, here are the details:\n",
    "    \n",
    "    Current Conversation Match: {current_conversation}\n",
    "    Previous Conversations: {' | '.join(previous_convos)}\n",
    "    What has worked well: {' '.join(what_worked)}\n",
    "    What to avoid: {' '.join(what_to_avoid)}\n",
    "    \n",
    "    Use these memories as context for your response to the user.\n",
    "    \n",
    "    Additionally, here are 10 guidelines for interactions with the current user: {procedural_memory}\"\"\"\n",
    "    \n",
    "    return SystemMessage(content=episodic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef9802-456a-4254-8210-161ad284726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procedural_memory_update(what_worked, what_to_avoid):\n",
    "\n",
    "    # Load Existing Procedural Memory Instructions\n",
    "    with open(\"./procedural_memory.txt\", \"r\") as content:\n",
    "        current_takeaways = content.read()\n",
    "\n",
    "    # Load Existing and Gathered Feedback into Prompt\n",
    "    procedural_prompt = f\"\"\"You are maintaining a continuously updated list of the most important procedural behavior instructions for an AI assistant. \n",
    "    Your task is to refine and improve a list of key takeaways based on new conversation feedback while maintaining the most valuable existing insights.\n",
    "\n",
    "    CURRENT TAKEAWAYS:\n",
    "    {current_takeaways}\n",
    "\n",
    "    NEW FEEDBACK:\n",
    "    What Worked Well:\n",
    "    {what_worked}\n",
    "\n",
    "    What To Avoid:\n",
    "    {what_to_avoid}\n",
    "\n",
    "    Please generate an updated list of up to 10 key takeaways that combines:\n",
    "    1. The most valuable insights from the current takeaways\n",
    "    2. New learnings from the recent feedback\n",
    "    3. Any synthesized insights combining multiple learnings\n",
    "\n",
    "    Requirements for each takeaway:\n",
    "    - Must be specific and actionable\n",
    "    - Should address a distinct aspect of behavior\n",
    "    - Include a clear rationale\n",
    "    - Written in imperative form (e.g., \"Maintain conversation context by...\")\n",
    "\n",
    "    Format each takeaway as:\n",
    "    [#]. [Instruction] - [Brief rationale]\n",
    "\n",
    "    The final list should:\n",
    "    - Be ordered by importance/impact\n",
    "    - Cover a diverse range of interaction aspects\n",
    "    - Focus on concrete behaviors rather than abstract principles\n",
    "    - Preserve particularly valuable existing takeaways\n",
    "    - Incorporate new insights when they provide meaningful improvements\n",
    "\n",
    "    Return up to but no more than 10 takeaways, replacing or combining existing ones as needed to maintain the most effective set of guidelines.\n",
    "    Return only the list, no preamble or explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate New Procedural Memory\n",
    "    procedural_memory = llm.invoke(procedural_prompt)\n",
    "\n",
    "    # Write to File\n",
    "    with open(\"./procedural_memory.txt\", \"w\") as content:\n",
    "        content.write(procedural_memory.content)\n",
    "\n",
    "    return\n",
    "\n",
    "# prompt = procedural_memory_update(what_worked, what_to_avoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0eadcd-ee4f-480f-9bae-28f4bb5dc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple storage for accumulated memories\n",
    "conversations = []\n",
    "what_worked = set()\n",
    "what_to_avoid = set()\n",
    "\n",
    "# Start Storage for Historical Message History\n",
    "messages = []\n",
    "\n",
    "while True:\n",
    "    # Get User's Message\n",
    "    user_input = input(\"\\nUser: \")\n",
    "    user_message = HumanMessage(content=user_input)\n",
    "    \n",
    "    # Generate new system prompt\n",
    "    system_prompt = episodic_system_prompt(user_input, vdb_client)\n",
    "    \n",
    "    # Reconstruct messages list with new system prompt first\n",
    "    messages = [\n",
    "        system_prompt,  # New system prompt always first\n",
    "        *[msg for msg in messages if not isinstance(msg, SystemMessage)]  # Old messages except system\n",
    "    ]\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        add_episodic_memory(messages, vdb_client)\n",
    "        print(\"\\n == Conversation Stored in Episodic Memory ==\")\n",
    "        procedural_memory_update(what_worked, what_to_avoid)\n",
    "        print(\"\\n== Procedural Memory Updated ==\")\n",
    "        break\n",
    "    if user_input.lower() == \"exit_quiet\":\n",
    "        print(\"\\n == Conversation Exited ==\")\n",
    "        break\n",
    "    \n",
    "    # Get context and add it as a temporary message\n",
    "    context_message = semantic_rag(user_input, vdb_client)\n",
    "    \n",
    "    # Pass messages + context + user input to LLM\n",
    "    response = llm.invoke([*messages, context_message, user_message])\n",
    "    print(\"\\nAI Message: \", response.content)\n",
    "    \n",
    "    # Add only the user message and response to permanent history\n",
    "    messages.extend([user_message, response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f7a73-4cf5-414b-8f59-8a7a6145dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_conversation(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83e46f-a349-4675-9b7f-b9388f192eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_prompt.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35038cec-8090-4411-91d2-a43ba3ca5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1099899-f95d-4386-96ee-5dece1bc5e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94230d38-fa59-4201-ad9f-9c5a0c1eee13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
